{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a517c21e-db6a-4060-940c-93779f683ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import imageio\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--gpuIDs', type=str, default='0', help='GPU IDs, e.g. \"0\" or \"0,1\"')\n",
    "    parser.add_argument('--scale', type=int, default=4)\n",
    "    parser.add_argument('--ngsrf', type=int, default=64, help='Base #channels for generatorSR')\n",
    "    parser.add_argument('--numResBlocks', type=int, default=8, help='Number of residual blocks in EDSR')\n",
    "    parser.add_argument('--batch_size', type=int, default=4)\n",
    "    parser.add_argument('--fine_size', type=int, default=100, help='Crop size for sub-volumes')\n",
    "    parser.add_argument('--itersPerEpoch', type=int, default=300)\n",
    "    parser.add_argument('--iterCyclesPerEpoch', type=int, default=1)\n",
    "    parser.add_argument('--valNum', type=int, default=10)\n",
    "    parser.add_argument('--valTest', action='store_true', help='If True, load separate test data for validation')\n",
    "    parser.add_argument('--epoch', type=int, default=1200)\n",
    "    parser.add_argument('--epoch_step', type=int, default=10)\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--print_freq', type=int, default=10)\n",
    "    parser.add_argument('--save_freq', type=int, default=10)\n",
    "    parser.add_argument('--dataset_dir', type=str, default='./Dataset/Bentheimer_mixed_fw90/Train')\n",
    "    parser.add_argument('--test_dir', type=str, default='./test_images/')\n",
    "    parser.add_argument('--test_save_dir', type=str, default='./results_final/')\n",
    "    parser.add_argument('--test_temp_save_dir', type=str, default='./results_temp/')\n",
    "    parser.add_argument('--modelName', type=str, default='DoubleSRNet')\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints_PyTorch')\n",
    "\n",
    "    parser.add_argument('--continue_train', action='store_true')\n",
    "    parser.add_argument('--phase', type=str, default='train', choices=['train','test'])\n",
    "    parser.add_argument('--continueEpoch', type=int, default=0)\n",
    "\n",
    "    parser.add_argument('--augFlag', action='store_true')\n",
    "    parser.add_argument('--distributed', action='store_true')\n",
    "    args = parser.parse_args([])\n",
    "    return args\n",
    "\n",
    "class DoubleSRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads entire HR.npy & LR.npy, randomly crops sub-slices of size:\n",
    "      (batch_size, 1, fine_size, fine_size) for LR\n",
    "      (batch_size, 1, fine_size*scale, fine_size*scale) for HR\n",
    "    Each dataset item is already a mini-batch => DataLoader batch_size=1\n",
    "    \"\"\"\n",
    "    def __init__(self, hr_path, lr_path,\n",
    "                 scale=4, fine_size=100,\n",
    "                 iters=300, batch_size=4,\n",
    "                 aug_flag=False):\n",
    "        super(DoubleSRDataset, self).__init__()\n",
    "        # load volumes\n",
    "        self.hr = np.load(hr_path)  # e.g. shape (1000,1000,1000)\n",
    "        self.lr = np.load(lr_path)  # e.g. shape (250,250,250)\n",
    "\n",
    "        self.scale = scale\n",
    "        self.fine_size = fine_size\n",
    "        self.iters = iters\n",
    "        self.batch_size = batch_size\n",
    "        self.aug_flag = aug_flag\n",
    "\n",
    "        # store (lr_batch, hr_batch) pairs in self.data_pairs\n",
    "        self.data_pairs = []\n",
    "        self.generate_subcubes()\n",
    "\n",
    "    def generate_subcubes(self):\n",
    "        for _ in range(self.iters):\n",
    "            lr_batch = []\n",
    "            hr_batch = []\n",
    "            for b in range(self.batch_size):\n",
    "                # random pick\n",
    "                x = np.random.randint(0, self.lr.shape[0] - self.fine_size)\n",
    "                y = np.random.randint(0, self.lr.shape[1] - self.fine_size)\n",
    "                z = np.random.randint(0, self.lr.shape[2])  # pick 1 slice in Z\n",
    "\n",
    "                # LR block\n",
    "                lr_block = self.lr[x:x+self.fine_size, y:y+self.fine_size, z]\n",
    "                lr_block = lr_block[None, ...]  # shape => (1, fine_size, fine_size)\n",
    "                lr_block = lr_block.astype(np.float32)/127.5 - 1.0\n",
    "\n",
    "                # HR block\n",
    "                X = x*self.scale\n",
    "                Y = y*self.scale\n",
    "                Z = z*self.scale  # if your HR is 4x in z dimension as well\n",
    "                hr_block = self.hr[X:X+self.fine_size*self.scale,\n",
    "                                   Y:Y+self.fine_size*self.scale,\n",
    "                                   Z]\n",
    "                hr_block = hr_block[None, ...]\n",
    "                hr_block = hr_block.astype(np.float32)/127.5 - 1.0\n",
    "\n",
    "                # optional augmentation\n",
    "                if self.aug_flag:\n",
    "                    if np.random.rand()<0.5:\n",
    "                        lr_block = lr_block[..., ::-1].copy()\n",
    "                        hr_block = hr_block[..., ::-1].copy()\n",
    "                    if np.random.rand()<0.5:\n",
    "                        lr_block = lr_block[..., ::-1, :].copy()\n",
    "                        hr_block = hr_block[..., ::-1, :].copy()\n",
    "\n",
    "                lr_batch.append(lr_block)\n",
    "                hr_batch.append(hr_block)\n",
    "\n",
    "            # stack into one mini-batch\n",
    "            lr_batch = np.stack(lr_batch, axis=0)  # (batch_size,1,H,W)\n",
    "            hr_batch = np.stack(hr_batch, axis=0)  # (batch_size,1,H*scale,W*scale)\n",
    "            self.data_pairs.append((lr_batch, hr_batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_batch, hr_batch = self.data_pairs[idx]\n",
    "        return torch.from_numpy(lr_batch), torch.from_numpy(hr_batch)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    def __init__(self, n_feats=64, kernel_size=3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_feats, n_feats, kernel_size, padding=kernel_size//2)\n",
    "        self.conv2 = nn.Conv2d(n_feats, n_feats, kernel_size, padding=kernel_size//2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        return out + identity\n",
    "\n",
    "def upsample_edsr_2d(x, scale):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if scale == 2:\n",
    "        # 2x\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "    elif scale == 3:\n",
    "        # 3x\n",
    "        x = F.interpolate(x, scale_factor=3, mode='nearest')\n",
    "    elif scale == 4:\n",
    "        # do two successive 2x\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unsupported scale in upsample_edsr_2d()\")\n",
    "    return x\n",
    "\n",
    "def upsample_edsr_1d(x, scale):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if scale == 2:\n",
    "        # 1 pass of (2,1)\n",
    "        x = F.interpolate(x, scale_factor=(2,1), mode='nearest')\n",
    "    elif scale == 3:\n",
    "        x = F.interpolate(x, scale_factor=(3,1), mode='nearest')\n",
    "    elif scale == 4:\n",
    "        # do two successive (2,1)\n",
    "        x = F.interpolate(x, scale_factor=(2,1), mode='nearest')\n",
    "        x = F.interpolate(x, scale_factor=(2,1), mode='nearest')\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unsupported scale in upsample_edsr_1d()\")\n",
    "    return x\n",
    "\n",
    "class EDSR(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, scale=4, n_resblocks=8, n_feats=64):\n",
    "        super(EDSR, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.n_resblocks = n_resblocks\n",
    "\n",
    "        self.head = nn.Conv2d(1, n_feats, kernel_size=3, padding=1)\n",
    "\n",
    "        body_blocks = []\n",
    "        for _ in range(n_resblocks):\n",
    "            body_blocks.append(ResidualBlock(n_feats=n_feats, kernel_size=3))\n",
    "        self.body = nn.Sequential(*body_blocks)\n",
    "\n",
    "        # final conv after upsampling\n",
    "        self.tail_conv = nn.Conv2d(n_feats, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1) main features\n",
    "        x_in = self.head(x)\n",
    "        res = self.body(x_in)\n",
    "        x_mid = x_in + res\n",
    "\n",
    "        # 2) upsample in 2D\n",
    "        x_up = upsample_edsr_2d(x_mid, self.scale)\n",
    "\n",
    "        # 3) final conv\n",
    "        out = self.tail_conv(x_up)\n",
    "\n",
    "        return torch.tanh(out)\n",
    "\n",
    "class EDSR1D(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, scale=4, n_resblocks=4, n_feats=32):\n",
    "        super(EDSR1D, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "        self.head = nn.Conv2d(1, n_feats, kernel_size=3, padding=1)\n",
    "\n",
    "        body_blocks = []\n",
    "        for _ in range(n_resblocks):\n",
    "            body_blocks.append(ResidualBlock(n_feats=n_feats, kernel_size=3))\n",
    "        self.body = nn.Sequential(*body_blocks)\n",
    "\n",
    "        self.tail_conv = nn.Conv2d(n_feats, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = self.head(x)\n",
    "        res = self.body(x_in)\n",
    "        x_mid = x_in + res\n",
    "\n",
    "        # upsample only one dimension\n",
    "        x_up = upsample_edsr_1d(x_mid, self.scale)\n",
    "\n",
    "        out = self.tail_conv(x_up)\n",
    "        return torch.tanh(out)\n",
    "\n",
    "\n",
    "def mean_absolute_error(pred, target):\n",
    "    return torch.mean(torch.abs(pred - target))\n",
    "\n",
    "def train_one_step(generatorSR, generatorSRC,\n",
    "                   optimizerSR, optimizerSRC,\n",
    "                   lr_batch, hr_batch,\n",
    "                   device='cuda'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    lr_batch = lr_batch.to(device)  # shape (B,1,H,W)\n",
    "    hr_batch = hr_batch.to(device)  # shape (B,1,H*scale, W*scale)\n",
    "\n",
    "    # pass 1: normal 2D\n",
    "    sr_xy = generatorSR(lr_batch)  # => (B,1,H*scale, W*scale)\n",
    "\n",
    "    # pass 2: transpose => 1D upsample\n",
    "    sr_xy_t = sr_xy.transpose(2,3)        # => (B,1,W*scale, H*scale)\n",
    "    hr_batch_t = hr_batch.transpose(2,3)  # same shape\n",
    "\n",
    "    sr_xyz = generatorSRC(sr_xy_t)        # => upscales one dim => (B,1,W*scale, H*scale *someFactor?)\n",
    "    # If scale=4 in EDSR1D, it might do a second 4x in the width dimension, leading to mismatch\n",
    "    # If you truly only want to upsample the \"Z\" axis, you'd store your slices differently.\n",
    "\n",
    "    # compare\n",
    "    loss1 = mean_absolute_error(sr_xy, hr_batch)\n",
    "    loss2 = mean_absolute_error(sr_xyz, hr_batch_t)\n",
    "    total_loss = loss1 + loss2\n",
    "\n",
    "    optimizerSR.zero_grad()\n",
    "    optimizerSRC.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizerSR.step()\n",
    "    optimizerSRC.step()\n",
    "\n",
    "    return loss1.item(), loss2.item()\n",
    "\n",
    "\n",
    "def main_train(args):\n",
    "    # set device\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpuIDs\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # build dataset\n",
    "    hr_path = os.path.join(args.dataset_dir, \"HR.npy\")\n",
    "    lr_path = os.path.join(args.dataset_dir, \"LR.npy\")\n",
    "    train_dataset = DoubleSRDataset(hr_path=hr_path,\n",
    "                                    lr_path=lr_path,\n",
    "                                    scale=args.scale,\n",
    "                                    fine_size=args.fine_size,\n",
    "                                    iters=args.itersPerEpoch,\n",
    "                                    batch_size=args.batch_size,\n",
    "                                    aug_flag=args.augFlag)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "    # create models\n",
    "    generatorSR = EDSR(scale=args.scale, n_resblocks=args.numResBlocks, n_feats=args.ngsrf).to(device)\n",
    "    # typically smaller for pass 2\n",
    "    generatorSRC = EDSR1D(scale=args.scale, n_resblocks=args.numResBlocks//2, n_feats=args.ngsrf//2).to(device)\n",
    "\n",
    "    optimizerSR = torch.optim.Adam(generatorSR.parameters(), lr=args.lr, betas=(0.9, 0.999))\n",
    "    optimizerSRC = torch.optim.Adam(generatorSRC.parameters(), lr=args.lr, betas=(0.9, 0.999))\n",
    "\n",
    "    # load checkpoints if continuing\n",
    "    if args.continue_train:\n",
    "        sr_ckpt = os.path.join(args.checkpoint_dir, f\"generatorSR_{args.continueEpoch}.pth\")\n",
    "        src_ckpt = os.path.join(args.checkpoint_dir, f\"generatorSRC_{args.continueEpoch}.pth\")\n",
    "        if os.path.exists(sr_ckpt):\n",
    "            generatorSR.load_state_dict(torch.load(sr_ckpt, map_location=device))\n",
    "            print(f\"Loaded generatorSR from epoch {args.continueEpoch}\")\n",
    "        if os.path.exists(src_ckpt):\n",
    "            generatorSRC.load_state_dict(torch.load(src_ckpt, map_location=device))\n",
    "            print(f\"Loaded generatorSRC from epoch {args.continueEpoch}\")\n",
    "\n",
    "    # optionally multi-gpu\n",
    "    if args.distributed and torch.cuda.device_count() > 1:\n",
    "        generatorSR = nn.DataParallel(generatorSR)\n",
    "        generatorSRC = nn.DataParallel(generatorSRC)\n",
    "\n",
    "    # simple LR schedule\n",
    "    def adjust_lr(optimizer, epoch_idx):\n",
    "        # replicate: lr = initial_lr * 0.5^(epoch / args.epoch_step)\n",
    "        new_lr = args.lr * (0.5 ** (epoch_idx / args.epoch_step))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "\n",
    "    global_step = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(args.continueEpoch, args.epoch):\n",
    "        # adjust lr\n",
    "        adjust_lr(optimizerSR, epoch)\n",
    "        adjust_lr(optimizerSRC, epoch)\n",
    "\n",
    "        epoch_loss1 = 0.\n",
    "        epoch_loss2 = 0.\n",
    "        step_count = 0\n",
    "\n",
    "        for i,(lr_batch, hr_batch) in enumerate(train_loader):\n",
    "            # shape: (1, batch_size,1,H,W)\n",
    "            lr_batch = lr_batch.squeeze(0)\n",
    "            hr_batch = hr_batch.squeeze(0)\n",
    "\n",
    "            l1, l2 = train_one_step(generatorSR, generatorSRC,\n",
    "                                    optimizerSR, optimizerSRC,\n",
    "                                    lr_batch, hr_batch,\n",
    "                                    device=device)\n",
    "            epoch_loss1 += l1\n",
    "            epoch_loss2 += l2\n",
    "            step_count += 1\n",
    "            global_step += 1\n",
    "\n",
    "            if (i+1) % args.print_freq == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{args.epoch}] Iter [{i+1}/{len(train_loader)}] \"\n",
    "                      f\"Loss1: {l1:.4f}, Loss2: {l2:.4f}, Time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "        epoch_loss1 /= step_count\n",
    "        epoch_loss2 /= step_count\n",
    "        print(f\"=== End Epoch {epoch+1} | Loss1: {epoch_loss1:.4f}, Loss2: {epoch_loss2:.4f} ===\")\n",
    "\n",
    "        if (epoch+1) % args.save_freq == 0:\n",
    "            os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
    "            sr_path = os.path.join(args.checkpoint_dir, f\"generatorSR_{epoch+1}.pth\")\n",
    "            src_path = os.path.join(args.checkpoint_dir, f\"generatorSRC_{epoch+1}.pth\")\n",
    "            torch.save(generatorSR.state_dict(), sr_path)\n",
    "            torch.save(generatorSRC.state_dict(), src_path)\n",
    "            print(f\"Saved checkpoint epoch {epoch+1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
